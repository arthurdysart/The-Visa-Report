{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Visa Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Arthur Dysart  \n",
    "Created on Sat Dec  1 14:12:09 2018\n",
    "\n",
    "Analyzes H1-B Visa data by most common \"Occupation\" and \"State.\" Reports are sorted by decreasing \"Certified Visas\" count and alphabetical \"Occupation\" title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path to input data file in system directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_path = r\"../input/H1B_FY_2014.csv\"\n",
    "occu_export_path = r\"../output/top_10_occupations.txt\"\n",
    "state_export_path = r\"../output/top_10_states.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the \"SparkSession\" class from the \"Spark SQL\" package. Create spark cluster connector object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"TheVisaReport\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the \"Pandas\" package for external data export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CSV source file as Spark dataframe object \"raw_df.\" Save imported data as SparkSQL table \"raw_data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.read.csv(import_path,\n",
    "                        sep = \";\",\n",
    "                        header = True,\n",
    "                        ignoreLeadingWhiteSpace = True,\n",
    "                        ignoreTrailingWhiteSpace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.createOrReplaceTempView(\"raw_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the following required columns in the input data file:\n",
    "* \"Status\" — identifies whether visa application is certified.\n",
    "* \"Occupation\" — identifies the occupation title for given visa application.\n",
    "* \"State\" — identifies job location (as U.S. state) for given visa application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_status = [col\n",
    "              for col\n",
    "              in all_columns\n",
    "              if 'STATUS' in col.upper()][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count total number of certified visas. Save count as SparkSQL table \"total_cert.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT\n",
    "            COUNT(status)\n",
    "                AS TOTAL_CERTIFIED_APPLICATIONS\n",
    "        FROM\n",
    "            raw_data\n",
    "        WHERE\n",
    "            status = 'CERTIFIED'\n",
    "        \"\"\"\n",
    "\n",
    "cert_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cert_df.createOrReplaceTempView(\"total_cert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform & load: top occupations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify column representing occupation name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_occupation = [col\n",
    "                  for col\n",
    "                  in all_columns\n",
    "                  if ('SOC' in col.upper() and\n",
    "                      'NAME' in col.upper())][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each occupation name:\n",
    "1. count visa applications with status \"certified,\" and\n",
    "2. calculate percentage of certified applications relative to total certified applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT\n",
    "            UPPER({})\n",
    "                AS TOP_OCCUPATIONS,\n",
    "            COUNT(status)\n",
    "                AS NUMBER_CERTIFIED_APPLICATIONS,\n",
    "            CONCAT(ROUND(COUNT(status) * 100 /(SELECT\n",
    "                                                   *\n",
    "                                               FROM\n",
    "                                                   total_cert), 1), \"%\")\n",
    "                AS PERCENTAGE\n",
    "        FROM\n",
    "            raw_data\n",
    "        WHERE\n",
    "            status = 'CERTIFIED'\n",
    "        GROUP BY\n",
    "            {}\n",
    "        ORDER BY\n",
    "            NUMBER_CERTIFIED_APPLICATIONS\n",
    "                DESC,\n",
    "            TOP_OCCUPATIONS\n",
    "                ASC\n",
    "        LIMIT\n",
    "            10\n",
    "        \"\"\"\\\n",
    "        .format(col_occupation,\n",
    "                col_occupation)\n",
    "\n",
    "occu_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine \"occu_df\" SparkSQL dataframe into one partition on master node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occu_df = occu_df.coalesce(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert \"occu_df\" SparkSQL dataframe to \"occu_pdf\" Pandas dataframe. Save result as TXT output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occu_pdf = occu_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occu_pdf.to_csv(occu_export_path,\n",
    "                sep = \";\",\n",
    "                index = False,\n",
    "                header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, export \"occu_df\" SparkSQL dataframe result in output directory as CSV.PART files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occu_df.write.csv(occu_export_path,\n",
    "                  sep = \";\",\n",
    "                  header = True,\n",
    "                  mode = \"errorifexists\",\n",
    "                  ignoreLeadingWhiteSpace = True,\n",
    "                  ignoreTrailingWhiteSpace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform & load: top states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify column representing state name of work location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_state = [col\n",
    "             for col\n",
    "             in all_columns\n",
    "             if ('WORK' in col.upper() and\n",
    "                 'STATE' in col.upper())][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each state:\n",
    "1. count visa applications with status \"certified,\" and\n",
    "2. calculate percentage of certified applications relative to total certified applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT\n",
    "            UPPER({})\n",
    "                AS TOP_STATES,\n",
    "            COUNT(status)\n",
    "                AS NUMBER_CERTIFIED_APPLICATIONS,\n",
    "            CONCAT(ROUND(COUNT(status) * 100 /(SELECT\n",
    "                                                   *\n",
    "                                               FROM\n",
    "                                                   total_cert), 1), \"%\")\n",
    "                AS PERCENTAGE\n",
    "        FROM\n",
    "            raw_data\n",
    "        WHERE\n",
    "            status = 'CERTIFIED'\n",
    "        GROUP BY\n",
    "            {}\n",
    "        ORDER BY\n",
    "            NUMBER_CERTIFIED_APPLICATIONS\n",
    "                DESC,\n",
    "            TOP_STATES\n",
    "                ASC\n",
    "        LIMIT\n",
    "            10\n",
    "        \"\"\"\\\n",
    "        .format(col_state,\n",
    "                col_state)\n",
    "\n",
    "state_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine \"state_df\" SparkSQL dataframe into one partition on master node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df = state_df.coalesce(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert \"state_df\" SparkSQL dataframe to \"state_pdf\" Pandas dataframe. Save result as TXT output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_pdf = state_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_pdf.to_csv(state_export_path,\n",
    "                 sep = \";\",\n",
    "                 index = False,\n",
    "                 header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, export \"state_df\" SparkSQL dataframe result in output directory as CSV.PART files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df.write.csv(state_export_path,\n",
    "                   sep = \";\",\n",
    "                   header = True,\n",
    "                   mode = \"errorifexists\",\n",
    "                   ignoreLeadingWhiteSpace = True,\n",
    "                   ignoreTrailingWhiteSpace = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
